{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsampling the Reddit Dataset\n",
    "The original reddit dataset is hosted on the HuggingFace Dataset Repository, an open-source directory in which any contributor can upload and share datasets for model training.  This dataset in particular is exceptionally large (20+GiB) so this notebook was created to provide transparency and document the steps we took to sample this down to something small enough to include with the Applied Machine Learning Prototype. \n",
    "\n",
    "**This notebook is NOT intended to be run from start to finish.** \n",
    "\n",
    "In fact, in its current state, that is not even possible as several the initial steps were actually completed on a different Colab notebook and other steps require intermediate files which are stored on a local machi.  However, we hope this provides a starting point for those who do wish to explore this dataset in more detail, as well as provide motivation for the subsample we ultimately include with the repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original reddit dataset has over 3.8 million posts from thousands of subreddits, which is sampled down to 600+K posts after several pre-processing steps: \n",
    "\n",
    "1. I first identify the most popular subreddits by number of posts. You can see the top 30 most popular subreddits\n",
    "2. I select two slightly different subsamples: \n",
    "    * **Top10**: select only those post from the top 10 subreddits by number of posts\n",
    "    * **Curated10**: select 10 subreddits by the semantic quality of the subreddit name\n",
    "3. Finally, I sampled within some subreddits: The top 3 subreddits (AskReddit, relationships, leagueoflegends) have an order of magnitude more posts than any other subreddit. To keep the subset manageable, I only include the first 60K posts from each of these subreddits. \n",
    "\n",
    "I keep both the **Top10** and **Curated10** as one dataset. Their is overlap between the subreddits contained in these two \"top 10\" lists for a total of just under 640K posts within 16 popular subreddits. \n",
    "\n",
    "In the cell below contains the original code from the Colab notebook that was used to perform this initial subsampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"reddit\")\n",
    "\n",
    "\"\"\"\n",
    "Downloading:\n",
    "4.38k/? [00:01<00:00, 3.32kB/s]\n",
    "\n",
    "Downloading:\n",
    "2.83k/? [00:00<00:00, 3.28kB/s]\n",
    "Using custom data configuration default\n",
    "\n",
    "Downloading and preparing dataset reddit/default (download: 2.93 GiB, generated: 17.64 GiB, post-processed: Unknown size, total: 20.57 GiB) to /root/.cache/huggingface/datasets/reddit/default/1.0.0/98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969...\n",
    "Downloading: 100%\n",
    "3.14G/3.14G [05:26<00:00, 9.62MB/s]\n",
    "\n",
    "3848330/0 [06:29<00:00, 10409.42 examples/s]\n",
    "Dataset reddit downloaded and prepared to /root/.cache/huggingface/datasets/reddit/default/1.0.0/98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969. Subsequent calls will reuse this data.\n",
    "\"\"\"\n",
    "\n",
    "dataset = dataset['train']\n",
    "\n",
    "# identify unique subreddits\n",
    "unique_subreddits, counts = np.unique(dataset['subreddit'], return_counts=True)\n",
    "\n",
    "# count posts in each subreddit; sort by post counts\n",
    "top_subreddits = [sr for c, sr in sorted(zip(counts, unique_subreddits), reverse=True)]\n",
    "\n",
    "top_subreddits[:30]\n",
    "#['AskReddit', 'relationships', 'leagueoflegends', 'tifu', 'relationship_advice', 'trees', 'gaming', 'atheism', 'AdviceAnimals', 'funny', 'politics', 'pics', 'sex', 'WTF', 'explainlikeimfive', 'todayilearned', 'Fitness', 'IAmA', 'worldnews', 'DotA2', 'TwoXChromosomes', 'videos', 'DestinyTheGame', 'reddit.com', 'offmychest', 'buildapc', 'AskMen', 'personalfinance', 'summonerschool', 'technology']\n",
    "\n",
    "# top10: select top10 subreddits accoring the number of posts\n",
    "top10_subreddits = top_subreddits[:10]\n",
    "# curated10: select 10 popular subreddits (in the top 30) that have semantically meaningful subreddit names\n",
    "curated_subreddits = ['relationships', 'trees', 'gaming', 'funny', 'politics', 'sex', 'Fitness', 'worldnews', 'personalfinance', 'technology']\n",
    "\n",
    "relevant_subreddits = list(set(top10_subreddits).union(set(curated_subreddits)))\n",
    "\n",
    "# next we create a mask that will select out posts from the relevant_subreddits\n",
    "subreddit_mask = np.zeros(len(dataset))\n",
    "\n",
    "# The top 3 categories each contain between 100k and 600k posts -- an order of magnitude more \n",
    "# than any other popular subreddit. \n",
    "# Truncate these 3 categories to a max of 60K posts each\n",
    "askreddit = 0\n",
    "relationships = 0\n",
    "lol = 0\n",
    "max_examples = 60000\n",
    "\n",
    "for i, sub in enumerate(dataset['subreddit']):\n",
    "  if sub in relevant_subreddits:\n",
    "    if sub == 'AskReddit' and askreddit <= max_examples:\n",
    "      subreddit_mask[i] = 1\n",
    "      askreddit += 1\n",
    "    elif sub == 'relationships' and relationships <= max_examples:\n",
    "      subreddit_mask[i] = 1\n",
    "      relationships += 1\n",
    "    elif sub == 'leagueoflegends' and lol <= max_examples:\n",
    "      subreddit_mask[i] = 1\n",
    "      lol += 1\n",
    "    elif sub not in ['AskReddit', 'relationships', 'leagueoflegends']:    \n",
    "      subreddit_mask[i] = 1\n",
    "    else:\n",
    "      continue\n",
    "\n",
    "subreddit_mask = subreddit_mask == 1\n",
    "\n",
    "np.sum(subreddit_mask)\n",
    "#636695 (reduced from over 3.8M!)\n",
    "\n",
    "# create the subset\n",
    "subset = dataset[subreddit_mask]\n",
    "\n",
    "# this file lives on a local machine\n",
    "subset = pd.DataFrame(subset)\n",
    "subset.to_csv(\"reddit_subset.pd\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Categorical columns for classification\n",
    "\n",
    "The fewshot library expects datasets to contain `category` and `label` columns. The `category` column should contain string descriptions, while the `label` column should contain integers, each unique to their respective category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filename = \"../my_data/reddit/reddit_subset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the subreddit names to a standardized format to create category names\n",
    "df['category'] = df['subreddit'].map({\n",
    "    'atheism': 'atheism',\n",
    "    'funny': 'funny',\n",
    "    'sex': 'sex',\n",
    "    'Fitness': 'fitness',\n",
    "    'AdviceAnimals': 'advice animals',\n",
    "    'trees': 'trees',\n",
    "    'personalfinance': 'personal finance', \n",
    "    'relationships': 'relationships',\n",
    "    'relationship_advice': 'relationship advice',\n",
    "    'tifu': 'tifu',\n",
    "    'politics': 'politics',\n",
    "    'gaming': 'gaming', \n",
    "    'worldnews': 'world news',\n",
    "    'technology': 'technology',\n",
    "    'leagueoflegends': 'league of legends',\n",
    "    'AskReddit': 'ask reddit'\n",
    "    })\n",
    "\n",
    "df['category'] = pd.Categorical(df.category)\n",
    "df['label'] = df.category.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# here we perform a stratified split on `subreddit` though\n",
    "# we could just as easily split on `category`\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df['subreddit'], \n",
    "                                                    test_size=.1, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=df['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that the number of examples in the training set is not fixed - the dataset is imbalanced. \n",
    "np.unique(X_train.subreddit, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(X_test.subreddit, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"../data/reddit/reddit_subset_train.csv\")\n",
    "X_test.to_csv(\"../data/reddit/reddit_subset_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which text should we use? \n",
    "You might have noticed earlier that there are three different text fields associated with a reddit post: the **content**, **body**, and **summary** columns. Through exploration I've noticed that the **content** and **body** columns are often very similar to each other. \n",
    "\n",
    "Below I create a figure that displays the distribution of character counts for each of these three columns for all reddit posts in the test set. I ultimately used the **summary** column as the data on which to classify for two reasons:\n",
    "1. fewer number of characters in each post mean faster processing and inference time through SentenceBERT\n",
    "2. the **summary** column is usually a TL;DR which could better encapsulate the idea behind the post and perhaps be more semantically meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def count_characters(x):\n",
    "    return len(x)\n",
    "\n",
    "plt.hist(X_test.body.apply(count_characters).values, alpha=.6, range=(0,6000), label='body');\n",
    "plt.hist(X_test.content.apply(count_characters).values, alpha=.6, range=(0,6000), label='content');\n",
    "plt.hist(X_test.summary.apply(count_characters).values, alpha=.6, range=(0,6000), label='summary');\n",
    "plt.legend()\n",
    "\n",
    "summary_lengths = X_test.summary.apply(count_characters).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(summary_lengths))\n",
    "print(np.median(summary_lengths))\n",
    "print(np.max(summary_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words in reddit dataset\n",
    "In our on-the-fly classification regime, we used the most frequently used words, as measured over the word2vec corpus, to create a mapping between SentenceBERT's embedding space and word2vec's embedding space. Another option is to instead use the most frequently used words as measured over the Reddit dataset! The core idea is that using words that are the most common to our dataset could improve the mapping between embedding spaces. \n",
    "\n",
    "In the following blocks, we perform this analysis and save that output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from nltk import FreqDist, word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ''\n",
    "\n",
    "for summary in df.summary:\n",
    "    try:\n",
    "        corpus += summary\n",
    "    except:\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop1 = list(string.punctuation) + [\"``\", \"''\", \"...\"] #\n",
    "stop2 = stopwords.words(\"english\") + list(string.punctuation) + [\"``\", \"''\", \"...\"]\n",
    "words1 = [word for word in thing if word not in stop1]\n",
    "words2 = [word for word in thing if word not in stop2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words1))\n",
    "print(len(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq1 = Counter(words1).most_common(100000)\n",
    "most_common_words1, counts = [list(c) for c in zip(*word_freq1)]\n",
    "\n",
    "word_freq2 = Counter(words2).most_common(100000)\n",
    "most_common_words2, counts = [list(c) for c in zip(*word_freq2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = {\"no_punc\": most_common_words1, \"no_punc_no_stop\": most_common_words2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(most_common_words, open(\"../data/reddit/most_common_words.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample the train set because it's too big\n",
    "Training set is *too big*?? Bet you've never heard that one before. However, in the few-shot analysis we're trying to explore regimes in which we don't have a lot of labeled examples (if any). So, in this case -- our training set is WAY too big!\n",
    "\n",
    "In the following cells we load the train set we created earlier, isolate just the **curated10** list of subreddits, and sample two sets of 10000 examples (1000 examples for each of the 10 categories). We save these two sets as the official train set and a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../my_data/reddit/reddit_subset_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_subreddits = ['relationships', 'trees', 'gaming', 'funny', 'politics', \\\n",
    "        'sex', 'Fitness', 'worldnews', 'personalfinance', 'technology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    df_train[df_train.subreddit.isin(curated_subreddits)]\n",
    "    .groupby('category', group_keys=False)\n",
    "    .apply(lambda x: x.sample(min(len(x), 2000), random_state=42))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(sample, sample['subreddit'], \n",
    "                                                      test_size=.5, \n",
    "                                                      random_state=42, \n",
    "                                                      stratify=sample['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby('subreddit')['subreddit'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"../data/reddit/reddit_subset_train1000.csv\")\n",
    "X_valid.to_csv(\"../data/reddit/reddit_subset_valid1000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subset of the reddit test set that has exactly 1300 examples per category\n",
    "\n",
    "Earlier we created a test set which was slightly imbalanced between the 16 classes. We decided to sample this test set down to make error analysis easier. We also focus solely on the **curated10** subreddits as classification performed better with these categories than with the **top10** categories (as expected). \n",
    "\n",
    "The final test set produced in this cell is included as an artifact in the fewshot repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fewshot.data.loaders import create_dataset_from_df \n",
    "\n",
    "df_test = pd.read_csv(\"../my_data/reddit/reddit_subset_test.csv\")\n",
    "\n",
    "# In our experiments we'll work with just 10 of the 16 most popular subreddits\n",
    "curated_subreddits = ['relationships', 'trees', 'gaming', 'funny', 'politics', \\\n",
    "      'sex', 'Fitness', 'worldnews', 'personalfinance', 'technology']\n",
    "\n",
    "df_reddit_test = (\n",
    "  df_test[df_test.subreddit.isin(curated_subreddits)]\n",
    "  .groupby('category', group_keys=False)\n",
    "  .apply(lambda x: x.sample(min(len(x), 1300), random_state=42))\n",
    "  .assign(\n",
    "      category = lambda df: pd.Categorical(df.category),\n",
    "      label = lambda df: df.category.cat.codes\n",
    "      )\n",
    "  )\n",
    "\n",
    "# the following line will compute all the SentenceBERT embeddings for each example\n",
    "# NOTE: only run this cell if you have a GPU or some time on your hands\n",
    "reddit_test_data = create_dataset_from_df(df_reddit_test, 'summary')\n",
    "torch_save(\"../data/reddit/reddit_dataset_1300.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If this documentation includes code, including but not limited to, code examples, Cloudera makes this available to you under the terms of the Apache License, Version 2.0, including any required notices.  A copy of the Apache License Version 2.0 can be found [here](https://opensource.org/licenses/Apache-2.0).***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
